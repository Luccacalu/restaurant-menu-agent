# ğŸ½ï¸ Restaurant Menu RAG â€“ Proof of Concept

This project is a **Proof of Concept (PoC)** and MVP of a **Retrieval-Augmented Generation (RAG)** system applied to a restaurant menu, which you can use to answer natural language questions about menu items, asking for recommendations, filtering by dietary preferences, ingredients, price, and more.

It demonstrates:
- Vector databases
- Embeddings
- LLM orchestration
- Structured query planning
- Intent extraction
- Semantic filtering and sorting
- End-to-end RAG architecture
- Admin vs User interfaces

---

## âœ¨ Overview

The system allows:

### ğŸ‘¨â€ğŸ³ Admin
- Add new menu items
- Edit existing items
- Remove items

### ğŸ‘¤ User
- Ask natural language questions about the menu
- Receive grounded answers based **only** on menu data
- Ask complex queries like:
  > â€œGive me the cheapest 5 vegan options with rice, no beans, that go well with wine. Also, I don't like bitter ingredients.â€

Menu items are embedded using **SentenceTransformers** and stored in **ChromaDB**.  
User questions are converted into a **query plan** that controls filtering, sorting, and semantic reasoning.

---

## ğŸ§  Architecture

### Admin flow
```
Admin UI
â†’ Menu item ingestion
â†’ SentenceTransformer generates embedding
â†’ ChromaDB upsert (document + metadata)
```

### User flow
```
User question
â†’ Query Planner (LLM)
â†’ Structured query plan (filters / extrema / semantic)
â†’ ChromaDB retrieval
â†’ Optional semantic reranking
â†’ RAG answer generation (LLM)
```

The system uses **two LLM stages**:
1. **Query planning** â€“ extracts structured intent (filters, sorting, semantic constraints)
2. **Answer generation** â€“ produces a grounded natural language answer

The `capabilities` field in the query plan determines which stages execute.

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit** â€“ Admin & User UI
- **SentenceTransformers** â€“ embeddings
- **ChromaDB** â€“ vector database (local, persistent)
- **Ollama** â€“ local LLM runtime
- **LLaMA 3.1 (8B)** â€“ language model

All inference runs **locally**. No external APIs are required.

---

## ğŸ“ Project Structure

```
core/
  embeddings.py        # Embedding model loader
  vectorstore.py       # ChromaDB access

domain/
  query_plan.py        # Query plan schema

models/
  menu_item.py         # Domain model

services/
  ingest_service.py        # Menu ingestion
  retrieval_service.py     # Candidate retrieval
  ingredient_filter_service.py
  semantic_rerank_service.py
  query_planner.py         # LLM-based planner
  query_execution_service.py
  rag_service.py           # RAG orchestration
  llm_service.py           # Ollama wrapper

scripts/
  bulk_ingest_menu.py      # Bulk menu ingestion

streamlit_app.py           # Streamlit interface
```

---

## ğŸ¤– LLM Setup (Ollama)

This project runs the LLM **locally**, without external APIs.

### 1ï¸âƒ£ Install Ollama
Download and install from:
```
https://ollama.com
```

### 2ï¸âƒ£ Pull the required model
```bash
ollama pull llama3.1:8b
```

### 3ï¸âƒ£ Start the Ollama service
```bash
ollama serve
```

---

## ğŸš€ Running the Application

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the Streamlit app
```bash
streamlit run streamlit_app.py
```

### 3ï¸âƒ£ Open in browser
```
http://localhost:8501
```

---

## ğŸ“¦ Bulk Menu Ingestion

```bash
python scripts/bulk_ingest_menu.py
```

This script:
- Supports large arrays of items
- Can be safely re-run to update existing items
- Uses the same ingestion pipeline as the admin UI

---

## âš ï¸ Notes & Limitations

- This is an MVP / PoC, not production-hardened
- ChromaDB uses local disk storage
- Ollama requires sufficient RAM (8GB recommended)

---
