# ğŸ½ï¸ Restaurant Menu RAG â€“ Proof of Concept

This project is a **Proof of Concept (PoC)** of a **Retrieval-Augmented Generation (RAG)** system applied to a restaurant menu.

It was built as a MVP to demonstrate:
- Vector databases
- Embeddings
- LLM orchestration
- End-to-end RAG architecture
- Admin vs User interfaces

---

## âœ¨ Overview

The system allows:

### ğŸ‘¨â€ğŸ³ Admin
- Add new menu items
- Edit existing items
- Remove items

### ğŸ‘¤ User
- Ask natural language questions about the menu
- Receive grounded answers based only on menu data

Menu items are embedded using **SentenceTransformers** and stored in **ChromaDB**.  
User questions are embedded and matched via **semantic search**, optionally constrained by **structured filters extracted by an LLM**.

---

## ğŸ§  Architecture

### Admin flow
```
Admin adds / edits menu item
â†’ SentenceTransformer generates embedding
â†’ ChromaDB upsert (document + metadata)
```

### User flow
```
User question
â†’ LLM extracts structured filters (diet, category, price)
â†’ SentenceTransformer embeds query
â†’ ChromaDB filtered vector search
â†’ LLM generates grounded answer
```

This design uses **two LLM layers**:
1. Filter extraction (structured reasoning)
2. Answer generation (natural language)

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit** â€“ UI for admin and user
- **SentenceTransformers** â€“ embeddings
- **ChromaDB** â€“ vector database
- **Ollama** â€“ local LLM runtime
- **LLaMA 3.1 (8B)** â€“ language model

---

## ğŸ“ Project Structure

```
core/
  embeddings.py        # Embedding model loader
  vectorstore.py       # ChromaDB access

models/
  menu_item.py         # Menu item model

services/
  ingest_service.py    # Menu ingestion (upsert)
  query_service.py     # Semantic search
  rag_service.py       # RAG orchestration
  llm_service.py       # LLM calls via Ollama

scripts/
  bulk_ingest_menu.py  # Bulk menu ingestion

streamlit_app.py       # Streamlit interface
```

---

## ğŸ¤– LLM Setup (Ollama)

This project runs the LLM **locally**, without external APIs.

### 1ï¸âƒ£ Install Ollama
Download and install from:
```
https://ollama.com
```

### 2ï¸âƒ£ Pull the required model
```bash
ollama pull llama3.1:8b
```

### 3ï¸âƒ£ Start the Ollama service
```bash
ollama serve
```

### LLM integration (code)

```python
# services/llm_service.py

import ollama

def generate_answer(prompt: str) -> str:
    response = ollama.chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response["message"]["content"]
```

---

## ğŸš€ Running the Application

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the Streamlit app
```bash
streamlit run streamlit_app.py
```

### 3ï¸âƒ£ Open in browser
```
http://localhost:8501
```

---

## ğŸ“¦ Bulk Menu Ingestion

```bash
python scripts/bulk_ingest_menu.py
```

This script:
- Supports large arrays of items
- Can be safely re-run to update existing items
